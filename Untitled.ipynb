{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fbb365-d35d-4aee-819e-7b9d844d5567",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "3. Workflow\n",
    "\n",
    "    Load Dataset\n",
    "\n",
    "        pd.read_csv()\n",
    "\n",
    "        Explore shape, head, describe, info\n",
    "\n",
    "    Preprocess Data\n",
    "\n",
    "        Handle missing or zero values (e.g., Glucose = 0)\n",
    "\n",
    "        Normalize/standardize if needed\n",
    "\n",
    "    EDA (Exploratory Data Analysis)\n",
    "\n",
    "        Correlation matrix\n",
    "\n",
    "        Distribution plots for features\n",
    "\n",
    "        Class balance (0: No diabetes, 1: Diabetic)\n",
    "\n",
    "    Split Data\n",
    "\n",
    "        train_test_split() (e.g., 80% train, 20% test)\n",
    "\n",
    "    Train Model\n",
    "\n",
    "        Use Logistic Regression, Decision Tree, or Random Forest\n",
    "\n",
    "    Evaluate Model\n",
    "\n",
    "        Accuracy, Confusion Matrix, Precision, Recall, F1 Score\n",
    "\n",
    "        classification_report and confusion_matrix\n",
    "\n",
    "    Bonus (Optional)\n",
    "\n",
    "        Create a simple interface using streamlit or gradio\n",
    "\n",
    "        Save model using joblib or pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65a5e9eb-243f-488a-bf33-9b2d38f0eb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a605cc97-c548-426c-9284-aff3ae3199fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DIABETES data sets in this directory are provided for use in 1994 \n",
      "AI in Medicine symposium submissions.  Permission is granted to use the\n",
      "data sets for other research purposes as long as appropriate credit is\n",
      "given as to the source (AIM-94 data set provided by Michael Kahn, MD, PhD, \n",
      "Washington University, St. Louis, MO).\n",
      "\n",
      "\n",
      "Index:\n",
      "------\n",
      "\n",
      "* Data-Codes: a listing of the codes used in the data sets.\n",
      "\n",
      "* Domain-Description: This file describes the basic physiology and patho-\n",
      "physiology of diabetes mellitus and its treatment.\n",
      "\n",
      "* data-[01-70]: data sets covering several weeks' to months' worth of\n",
      "outpatient care on 70 patients.  An additional 10 sets will be made\n",
      "available two weeks prior to the symposium for interested parties.  Please\n",
      "contact the organizers if you would like to obtain these data sets.\n",
      "\n",
      "\n",
      "Methods:\n",
      "--------\n",
      "\n",
      "You do not need to use all the data in order to participate.  Use any \n",
      "subset of the available data from either the ICU data set or the diabetes \n",
      "data set.  Furthermore, do not feel constrained if your methods cannot \n",
      "be applied directly to these data sets.  We will consider submissions\n",
      "on related (i.e., clinical data interpretation) topics.  If in doubt,\n",
      "consult with us via e-mail at <aim-94@camis.stanford.edu>.\n",
      "\n",
      "\n",
      "The 'Matchmaker' Service:\n",
      "-------------------------\n",
      "\n",
      "We realize that an accurate interpretation of clinical data requires \n",
      "a thorough understanding of the physiological principles and clinical \n",
      "issues involved.  We also realize that many AIM researchers do not \n",
      "have convenient access to medical expertise, and that a symposium \n",
      "focusing on a clinical theme may catch several parties at a disadvantage.  \n",
      "Conversely, some clinical researchers may be interested in participating \n",
      "but may not have collaborators on the computer science end of the field.  \n",
      "To offset such disadvantages, we will provide a simple 'Matchmaker' \n",
      "service for AIM-94.  The purpose of this service is to establish a medium \n",
      "by which researchers can seek collaborators of complementary background \n",
      "and interests for AIM-94 participation and beyond.\n",
      "\n",
      "If you are interested in participating in this program, send a\n",
      "one-paragraph description of your background, research interests, and the\n",
      "type of collaboration you are pursuing to <aim-94@camis.stanford.edu> by\n",
      "September 20th.  We will collate these entries and distribute the whole\n",
      "list to all participants of the program.  It will be the participants' \n",
      "responsibility to contact others to discuss and establish collaborative \n",
      "efforts; AIM-94 organizers will solely act as mediators.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"README-DIABETES\", \"r\") as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41891b40-32ec-434d-941c-6fe82fc44992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.git', '.ipynb_checkpoints', 'data-01', 'data-02', 'data-03', 'data-04', 'data-05', 'data-06', 'data-07', 'data-08', 'data-09', 'data-10', 'data-11', 'data-12', 'data-13', 'data-14', 'data-15', 'data-16', 'data-17', 'data-18', 'data-19', 'data-20', 'data-21', 'data-22', 'data-23', 'data-24', 'data-25', 'data-26', 'data-27', 'data-28', 'data-29', 'data-30', 'data-31', 'data-32', 'data-33', 'data-34', 'data-35', 'data-36', 'data-37', 'data-38', 'data-39', 'data-40', 'data-41', 'data-42', 'data-43', 'data-44', 'data-45', 'data-46', 'data-47', 'data-48', 'data-49', 'data-50', 'data-51', 'data-52', 'data-53', 'data-54', 'data-55', 'data-56', 'data-57', 'data-58', 'data-59', 'data-60', 'data-61', 'data-62', 'data-63', 'data-64', 'data-65', 'data-66', 'data-67', 'data-68', 'data-69', 'data-70', 'Data-Codes', 'Domain-Description', 'Index', 'README-DIABETES', 'README.md', 'Untitled.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir())  # Lists all files in current directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac6bbca3-4c2b-479d-b0e0-c3c947483a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-21-1991\t9:09\t58\t100\n",
      "\n",
      "04-21-1991\t9:09\t33\t009\n",
      "\n",
      "04-21-1991\t9:09\t34\t013\n",
      "\n",
      "04-21-1991\t17:08\t62\t119\n",
      "\n",
      "04-21-1991\t17:08\t33\t007\n",
      "\n",
      "04-21-1991\t22:51\t48\t123\n",
      "\n",
      "04-22-1991\t7:35\t58\t216\n",
      "\n",
      "04-22-1991\t7:35\t33\t010\n",
      "\n",
      "04-22-1991\t7:35\t34\t013\n",
      "\n",
      "04-22-1991\t13:40\t33\t002\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"data-01\", \"r\") as file:\n",
    "    for _ in range(10):\n",
    "        print(file.readline())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2177dfe-31ab-4db6-875f-e7ebfab73ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date   Time  Code1  Code2\n",
      "0  04-21-1991   9:09     58    100\n",
      "1  04-21-1991   9:09     33      9\n",
      "2  04-21-1991   9:09     34     13\n",
      "3  04-21-1991  17:08     62    119\n",
      "4  04-21-1991  17:08     33      7\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data-01\", sep=\"\\t\", header=None)\n",
    "df.columns = [\"Date\", \"Time\", \"Code1\", \"Code2\"]  # Naming columns based on observation\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f596db9c-90a0-4d11-89f5-437c81fbb2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data-Codes: shape = (39, 1)\n",
      "data-01: shape = (943, 4)\n",
      "data-02: shape = (761, 4)\n",
      "data-03: shape = (300, 4)\n",
      "data-04: shape = (300, 4)\n",
      "data-05: shape = (300, 4)\n",
      "data-06: shape = (149, 4)\n",
      "data-07: shape = (242, 4)\n",
      "data-08: shape = (177, 4)\n",
      "data-09: shape = (206, 4)\n",
      "data-10: shape = (247, 4)\n",
      "data-11: shape = (236, 4)\n",
      "data-12: shape = (300, 4)\n",
      "data-13: shape = (300, 4)\n",
      "data-14: shape = (230, 4)\n",
      "data-15: shape = (300, 4)\n",
      "data-16: shape = (300, 4)\n",
      "data-17: shape = (251, 4)\n",
      "data-18: shape = (300, 4)\n",
      "data-19: shape = (300, 4)\n",
      "data-20: shape = (1003, 4)\n",
      "data-21: shape = (517, 4)\n",
      "data-22: shape = (300, 4)\n",
      "data-23: shape = (300, 4)\n",
      "data-24: shape = (300, 4)\n",
      "data-25: shape = (110, 4)\n",
      "data-26: shape = (483, 4)\n",
      "data-27: shape = (926, 4)\n",
      "data-28: shape = (951, 4)\n",
      "data-29: shape = (1289, 4)\n",
      "data-30: shape = (1179, 4)\n",
      "data-31: shape = (670, 4)\n",
      "data-32: shape = (157, 4)\n",
      "data-33: shape = (300, 4)\n",
      "data-34: shape = (300, 4)\n",
      "data-35: shape = (300, 4)\n",
      "data-36: shape = (265, 4)\n",
      "data-37: shape = (300, 4)\n",
      "data-38: shape = (300, 4)\n",
      "data-39: shape = (300, 4)\n",
      "data-40: shape = (106, 4)\n",
      "data-41: shape = (494, 4)\n",
      "data-42: shape = (541, 4)\n",
      "data-43: shape = (300, 4)\n",
      "data-44: shape = (300, 4)\n",
      "data-45: shape = (300, 4)\n",
      "data-46: shape = (300, 4)\n",
      "data-47: shape = (300, 4)\n",
      "data-48: shape = (300, 4)\n",
      "data-49: shape = (240, 4)\n",
      "data-50: shape = (300, 4)\n",
      "data-51: shape = (300, 4)\n",
      "data-52: shape = (300, 4)\n",
      "data-53: shape = (297, 4)\n",
      "data-54: shape = (779, 4)\n",
      "data-55: shape = (1327, 4)\n",
      "data-56: shape = (1018, 4)\n",
      "data-57: shape = (133, 4)\n",
      "data-58: shape = (300, 4)\n",
      "data-59: shape = (300, 4)\n",
      "data-60: shape = (300, 4)\n",
      "data-61: shape = (300, 4)\n",
      "data-62: shape = (300, 4)\n",
      "data-63: shape = (300, 4)\n",
      "data-64: shape = (86, 4)\n",
      "data-65: shape = (1126, 4)\n",
      "data-66: shape = (239, 4)\n",
      "data-67: shape = (967, 4)\n",
      "data-68: shape = (693, 4)\n",
      "data-69: shape = (51, 4)\n",
      "data-70: shape = (341, 4)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "files = sorted(glob.glob(\"data-*\"))\n",
    "\n",
    "for file in files:\n",
    "    temp_df = pd.read_csv(file, sep=\"\\t\", header=None)\n",
    "    print(f\"{file}: shape = {temp_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bdf09cf-126c-47fe-a0fb-7d638a3b1fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29285, 3)\n",
      "   Code1 Code2            Datetime\n",
      "0     58   100 1991-04-21 09:09:00\n",
      "1     33     9 1991-04-21 09:09:00\n",
      "2     34    13 1991-04-21 09:09:00\n",
      "3     62   119 1991-04-21 17:08:00\n",
      "4     33     7 1991-04-21 17:08:00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Pattern to match only data files with numbers\n",
    "files = sorted(glob.glob(\"data-[0-9][0-9]*\"))\n",
    "\n",
    "dfs = []\n",
    "for file in files:\n",
    "    temp_df = pd.read_csv(file, sep=\"\\t\", header=None)\n",
    "    temp_df.columns = [\"Date\", \"Time\", \"Code1\", \"Code2\"]\n",
    "    temp_df['Datetime'] = pd.to_datetime(temp_df['Date'] + ' ' + temp_df['Time'], format='%m-%d-%Y %H:%M', errors='coerce')\n",
    "    temp_df = temp_df.drop(columns=['Date', 'Time'])\n",
    "    dfs.append(temp_df)\n",
    "\n",
    "full_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Optional: remove rows with invalid datetime\n",
    "full_df = full_df.dropna(subset=['Datetime'])\n",
    "\n",
    "print(full_df.shape)\n",
    "print(full_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f834ced-9cd8-4766-8464-ac083e021e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
